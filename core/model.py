from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import re

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

PERSONALITY = """
You are Abd al-Rahman, an intelligent assistant who speaks and thinks like Mahmoud.
You are sharp, strategic, honest, and to-the-point. You avoid fluff.
You speak clearly in Arabic or English as needed, with warmth but precision.
You prefer concise and deep responses and always aim for meaningful output.
"""

def load_qwen():
    model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen-7B-Chat",
        trust_remote_code=True,
        torch_dtype=torch.bfloat16  # Ø§Ø³ØªØ®Ø¯Ù… bf16 Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ GPU
    ).to(device)  # ØªØ£ÙƒØ¯ Ø£Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù†ÙÙ‚Ù„ Ø¥Ù„Ù‰ GPU
    tokenizer = AutoTokenizer.from_pretrained(
        "Qwen/Qwen-7B-Chat",
        trust_remote_code=True
    )
    return model, tokenizer

def model_chat_with_qwen(model, tokenizer, prompt):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    # Ø·Ø¨Ø§Ø¹Ø© Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù„ØªØªØ¨Ø¹ Ø§Ù„Ø£Ø¯Ø§Ø¡ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
    print(f"ğŸ’¾ Allocated: {torch.cuda.memory_allocated() / 1024 ** 2:.2f} MB")
    print(f"ğŸ’¾ Reserved: {torch.cuda.memory_reserved() / 1024 ** 2:.2f} MB")

    outputs = model.generate(
        **inputs,
        max_new_tokens=32,  # Ù‚Ù„Ù„ Ø¹Ø¯Ø¯ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…Ù†ØªØ¬Ø©
        temperature=0.7,
        top_p=0.9,
        top_k=0,
        do_sample=True
    )
    torch.cuda.empty_cache()  # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¨Ø¹Ø¯ Ø§Ù„ØªÙˆÙ„ÙŠØ¯

    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Ù†Ø­Ø§ÙˆÙ„ Ø¥ÙŠØ¬Ø§Ø¯ Ø¢Ø®Ø± Ø±Ø¯ Ù…Ù† assistant
    matches = re.findall(r"assistant:\s*(.*)", full_output, re.DOTALL)
    if matches:
        return matches[-1].strip()

    # fallback: Ø¨ØµÙŠØºØ© ChatML
    match = re.search(r"<\|assistant\|>\s*(.*)", full_output, re.DOTALL)
    if match:
        return match.group(1).strip()

    # fallback: Ø¢Ø®Ø± Ø³Ø·Ø± ÙÙ‚Ø·
    return full_output.strip().split("\n")[-1]

